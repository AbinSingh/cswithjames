{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 300\n",
    "l = 10\n",
    "num_filter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input Data, value in between 0 and 1\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input):\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(relu)\n",
    "        Dropout_Layer = Dropout(0.2)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Dropout_Layer])\n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transition(input):\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(num_filter, (1,1), use_bias=False ,padding='same')(relu)\n",
    "    Dropout_Layer = Dropout(0.2)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Dropout_Layer)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEpoch 50/50\\nloss: 0.0695 - acc: 0.9756 - val_loss: 1.1741 - val_acc: 0.7874\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D)\n",
    "First_Transition = add_transition(First_Block)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition)\n",
    "Second_Transition = add_transition(Second_Block)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition)\n",
    "Third_Transition = add_transition(Third_Block)\n",
    "\n",
    "Last_Block = add_denseblock(Third_Transition)\n",
    "output = output_layer(Last_Block)\n",
    "\n",
    "'''\n",
    "Epoch 50/50\n",
    "loss: 0.0695 - acc: 0.9756 - val_loss: 1.1741 - val_acc: 0.7874\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 20)   540         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 20)   80          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 20)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 20)   3600        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 20)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 40)   0           conv2d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 40)   160         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 40)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 20)   7200        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 20)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 60)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 60)   240         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 60)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 20)   10800       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 20)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 80)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 80)   320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 20)   14400       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 20)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 100)  0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 100)  400         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 100)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 20)   18000       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 20)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 120)  0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 120)  480         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 120)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 20)   21600       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 20)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 140)  0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 140)  560         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 140)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 20)   25200       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 20)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 160)  0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 20)   28800       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 20)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 180)  0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 180)  720         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 180)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 20)   32400       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 32, 20)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 200)  0           concatenate_8[0][0]              \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 200)  800         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 200)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 20)   36000       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 32, 20)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 220)  0           concatenate_9[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 220)  880         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 220)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 20)   4400        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 20)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 20)   0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 20)   80          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 20)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 20)   3600        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 16, 20)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 40)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 40)   160         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 40)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 20)   7200        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 16, 16, 20)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 60)   0           concatenate_11[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 60)   240         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 60)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 20)   10800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 16, 20)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 80)   0           concatenate_12[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 80)   320         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 80)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 20)   14400       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16, 16, 20)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 100)  0           concatenate_13[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 100)  400         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 100)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 20)   18000       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 20)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 120)  0           concatenate_14[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 120)  480         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 120)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 20)   21600       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 20)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 140)  0           concatenate_15[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 140)  560         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 140)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 20)   25200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 20)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 160)  0           concatenate_16[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 160)  640         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 160)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 20)   28800       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 20)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 180)  0           concatenate_17[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 180)  720         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 180)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 20)   32400       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16, 16, 20)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 200)  0           concatenate_18[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 200)  800         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 200)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 20)   36000       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16, 16, 20)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 220)  0           concatenate_19[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 220)  880         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 220)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 20)   4400        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16, 16, 20)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 20)     0           dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 20)     80          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 20)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 20)     3600        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 8, 8, 20)     0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 40)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 40)     160         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 40)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 20)     7200        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 8, 8, 20)     0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 8, 8, 60)     0           concatenate_21[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 60)     240         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 60)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 20)     10800       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 8, 8, 20)     0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 8, 8, 80)     0           concatenate_22[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 80)     320         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 80)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 20)     14400       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 8, 8, 20)     0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 100)    0           concatenate_23[0][0]             \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 100)    400         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 100)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 20)     18000       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 8, 8, 20)     0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 120)    0           concatenate_24[0][0]             \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 120)    480         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 120)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 20)     21600       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 8, 8, 20)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 140)    0           concatenate_25[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 140)    560         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 140)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 20)     25200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 8, 8, 20)     0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 160)    0           concatenate_26[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 160)    640         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 160)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 20)     28800       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 8, 8, 20)     0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 180)    0           concatenate_27[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 180)    720         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 180)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 20)     32400       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 8, 8, 20)     0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 200)    0           concatenate_28[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 200)    800         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 200)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 20)     36000       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 8, 8, 20)     0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 220)    0           concatenate_29[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 220)    880         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 220)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 20)     4400        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 8, 8, 20)     0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 4, 4, 20)     0           dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 4, 20)     80          average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 20)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 20)     3600        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 4, 4, 20)     0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 4, 4, 40)     0           average_pooling2d_3[0][0]        \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4, 4, 40)     160         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 40)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 20)     7200        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 4, 4, 20)     0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 4, 4, 60)     0           concatenate_31[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 4, 4, 60)     240         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 60)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 4, 4, 20)     10800       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 4, 4, 20)     0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 4, 4, 80)     0           concatenate_32[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 4, 4, 80)     320         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 80)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 4, 20)     14400       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 4, 4, 20)     0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 4, 4, 100)    0           concatenate_33[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 4, 4, 100)    400         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 100)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 4, 20)     18000       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 4, 4, 20)     0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 4, 4, 120)    0           concatenate_34[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 4, 4, 120)    480         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 120)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 4, 4, 20)     21600       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 4, 4, 20)     0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 4, 4, 140)    0           concatenate_35[0][0]             \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 140)    560         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 140)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 20)     25200       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 4, 4, 20)     0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 4, 4, 160)    0           concatenate_36[0][0]             \n",
      "                                                                 dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 4, 4, 160)    640         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 160)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 20)     28800       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 4, 4, 20)     0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 4, 4, 180)    0           concatenate_37[0][0]             \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 4, 4, 180)    720         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 180)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 20)     32400       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 4, 4, 20)     0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 4, 4, 200)    0           concatenate_38[0][0]             \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 200)    800         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 200)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 20)     36000       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 4, 4, 20)     0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 4, 4, 220)    0           concatenate_39[0][0]             \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 220)    880         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 220)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 2, 2, 220)    0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 880)          0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           8810        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 835,670\n",
      "Trainable params: 825,110\n",
      "Non-trainable params: 10,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "50000/50000 [==============================] - 235s 5ms/step - loss: 1.4451 - acc: 0.4676 - val_loss: 2.8243 - val_acc: 0.2559\n",
      "Epoch 2/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 1.0188 - acc: 0.6333 - val_loss: 1.1735 - val_acc: 0.6198\n",
      "Epoch 3/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.8296 - acc: 0.7039 - val_loss: 1.0868 - val_acc: 0.6628\n",
      "Epoch 4/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.7309 - acc: 0.7385 - val_loss: 1.9132 - val_acc: 0.5308\n",
      "Epoch 5/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.6572 - acc: 0.7678 - val_loss: 0.8317 - val_acc: 0.7314\n",
      "Epoch 6/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.5990 - acc: 0.7871 - val_loss: 1.5741 - val_acc: 0.5828\n",
      "Epoch 7/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.5493 - acc: 0.8058 - val_loss: 0.8642 - val_acc: 0.7271\n",
      "Epoch 8/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.5146 - acc: 0.8205 - val_loss: 0.9467 - val_acc: 0.7322\n",
      "Epoch 9/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.4824 - acc: 0.8307 - val_loss: 0.7543 - val_acc: 0.7724\n",
      "Epoch 10/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.4605 - acc: 0.8394 - val_loss: 0.7295 - val_acc: 0.7834\n",
      "Epoch 11/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.4326 - acc: 0.8478 - val_loss: 0.8314 - val_acc: 0.7767\n",
      "Epoch 12/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.4116 - acc: 0.8548 - val_loss: 0.8409 - val_acc: 0.7637\n",
      "Epoch 13/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.3919 - acc: 0.8632 - val_loss: 0.7567 - val_acc: 0.7826\n",
      "Epoch 14/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.3706 - acc: 0.8702 - val_loss: 1.1742 - val_acc: 0.7046\n",
      "Epoch 15/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.3504 - acc: 0.8755 - val_loss: 1.0655 - val_acc: 0.7247\n",
      "Epoch 16/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.3411 - acc: 0.8804 - val_loss: 1.0463 - val_acc: 0.7269\n",
      "Epoch 17/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.3246 - acc: 0.8853 - val_loss: 0.5591 - val_acc: 0.8319\n",
      "Epoch 18/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.3090 - acc: 0.8895 - val_loss: 0.6818 - val_acc: 0.8172\n",
      "Epoch 19/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.2983 - acc: 0.8943 - val_loss: 0.5335 - val_acc: 0.8429\n",
      "Epoch 20/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.2848 - acc: 0.8989 - val_loss: 0.7950 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.2735 - acc: 0.9034 - val_loss: 0.8812 - val_acc: 0.7757\n",
      "Epoch 22/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.2625 - acc: 0.9079 - val_loss: 0.5294 - val_acc: 0.8523\n",
      "Epoch 23/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.2497 - acc: 0.9124 - val_loss: 0.7265 - val_acc: 0.8042\n",
      "Epoch 24/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.2475 - acc: 0.9116 - val_loss: 0.7358 - val_acc: 0.8009\n",
      "Epoch 25/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.2376 - acc: 0.9146 - val_loss: 0.7513 - val_acc: 0.8100\n",
      "Epoch 26/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.2301 - acc: 0.9180 - val_loss: 0.7063 - val_acc: 0.8318\n",
      "Epoch 27/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.2184 - acc: 0.9232 - val_loss: 0.5710 - val_acc: 0.8491\n",
      "Epoch 28/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.2097 - acc: 0.9234 - val_loss: 0.8426 - val_acc: 0.8043\n",
      "Epoch 29/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.2016 - acc: 0.9282 - val_loss: 0.5728 - val_acc: 0.8554\n",
      "Epoch 30/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.2004 - acc: 0.9286 - val_loss: 0.7541 - val_acc: 0.8284\n",
      "Epoch 31/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.1928 - acc: 0.9302 - val_loss: 0.6440 - val_acc: 0.8432\n",
      "Epoch 32/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.1895 - acc: 0.9315 - val_loss: 0.6933 - val_acc: 0.8237\n",
      "Epoch 33/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.1820 - acc: 0.9348 - val_loss: 0.5654 - val_acc: 0.8621\n",
      "Epoch 34/300\n",
      "50000/50000 [==============================] - 213s 4ms/step - loss: 0.1694 - acc: 0.9386 - val_loss: 0.6625 - val_acc: 0.8462\n",
      "Epoch 35/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1680 - acc: 0.9402 - val_loss: 0.6725 - val_acc: 0.8425\n",
      "Epoch 36/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1634 - acc: 0.9414 - val_loss: 0.6464 - val_acc: 0.8507\n",
      "Epoch 37/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.1590 - acc: 0.9419 - val_loss: 0.6278 - val_acc: 0.8528\n",
      "Epoch 38/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1578 - acc: 0.9428 - val_loss: 0.5740 - val_acc: 0.8637\n",
      "Epoch 39/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1519 - acc: 0.9451 - val_loss: 0.6802 - val_acc: 0.8470\n",
      "Epoch 40/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1446 - acc: 0.9491 - val_loss: 0.6902 - val_acc: 0.8424\n",
      "Epoch 41/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.1425 - acc: 0.9495 - val_loss: 0.6126 - val_acc: 0.8616\n",
      "Epoch 42/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.1351 - acc: 0.9499 - val_loss: 0.7020 - val_acc: 0.8441\n",
      "Epoch 43/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1336 - acc: 0.9521 - val_loss: 0.6053 - val_acc: 0.8627\n",
      "Epoch 44/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1319 - acc: 0.9527 - val_loss: 0.5475 - val_acc: 0.8717\n",
      "Epoch 45/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1268 - acc: 0.9545 - val_loss: 0.6607 - val_acc: 0.8466\n",
      "Epoch 46/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1251 - acc: 0.9533 - val_loss: 0.5554 - val_acc: 0.8708\n",
      "Epoch 47/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.1201 - acc: 0.9575 - val_loss: 0.7248 - val_acc: 0.8432\n",
      "Epoch 48/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.1199 - acc: 0.9562 - val_loss: 0.5787 - val_acc: 0.8723\n",
      "Epoch 49/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1162 - acc: 0.9579 - val_loss: 0.8131 - val_acc: 0.8359\n",
      "Epoch 50/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1168 - acc: 0.9582 - val_loss: 0.6890 - val_acc: 0.8434\n",
      "Epoch 51/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.1161 - acc: 0.9577 - val_loss: 0.6115 - val_acc: 0.8617\n",
      "Epoch 52/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1078 - acc: 0.9605 - val_loss: 0.6548 - val_acc: 0.8585\n",
      "Epoch 53/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1072 - acc: 0.9619 - val_loss: 0.5495 - val_acc: 0.8777\n",
      "Epoch 54/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.1085 - acc: 0.9604 - val_loss: 0.6182 - val_acc: 0.8643\n",
      "Epoch 55/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.1023 - acc: 0.9630 - val_loss: 0.7747 - val_acc: 0.8410\n",
      "Epoch 56/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1013 - acc: 0.9638 - val_loss: 0.6701 - val_acc: 0.8559\n",
      "Epoch 57/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.1021 - acc: 0.9639 - val_loss: 0.5700 - val_acc: 0.8747\n",
      "Epoch 58/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0972 - acc: 0.9650 - val_loss: 0.6604 - val_acc: 0.8626\n",
      "Epoch 59/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0961 - acc: 0.9654 - val_loss: 0.7760 - val_acc: 0.8436\n",
      "Epoch 60/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0939 - acc: 0.9668 - val_loss: 0.9277 - val_acc: 0.8198\n",
      "Epoch 61/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0905 - acc: 0.9673 - val_loss: 0.6941 - val_acc: 0.8579\n",
      "Epoch 62/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0917 - acc: 0.9670 - val_loss: 0.7047 - val_acc: 0.8551\n",
      "Epoch 63/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0902 - acc: 0.9676 - val_loss: 0.5544 - val_acc: 0.8799\n",
      "Epoch 64/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0912 - acc: 0.9666 - val_loss: 0.6549 - val_acc: 0.8688\n",
      "Epoch 65/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0860 - acc: 0.9696 - val_loss: 0.6635 - val_acc: 0.8660\n",
      "Epoch 66/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0852 - acc: 0.9691 - val_loss: 0.8020 - val_acc: 0.8433\n",
      "Epoch 67/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0878 - acc: 0.9675 - val_loss: 0.7558 - val_acc: 0.8436\n",
      "Epoch 68/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0853 - acc: 0.9705 - val_loss: 0.7637 - val_acc: 0.8508\n",
      "Epoch 69/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0795 - acc: 0.9714 - val_loss: 0.7383 - val_acc: 0.8480\n",
      "Epoch 70/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0756 - acc: 0.9726 - val_loss: 0.6235 - val_acc: 0.8729\n",
      "Epoch 71/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0808 - acc: 0.9711 - val_loss: 0.6510 - val_acc: 0.8666\n",
      "Epoch 72/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0782 - acc: 0.9717 - val_loss: 0.7327 - val_acc: 0.8572\n",
      "Epoch 73/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0770 - acc: 0.9723 - val_loss: 0.6397 - val_acc: 0.8676\n",
      "Epoch 74/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0765 - acc: 0.9723 - val_loss: 0.7101 - val_acc: 0.8622\n",
      "Epoch 75/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0730 - acc: 0.9744 - val_loss: 0.7016 - val_acc: 0.8669\n",
      "Epoch 76/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0738 - acc: 0.9731 - val_loss: 0.7922 - val_acc: 0.8515\n",
      "Epoch 77/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0767 - acc: 0.9729 - val_loss: 0.6110 - val_acc: 0.8778\n",
      "Epoch 78/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0735 - acc: 0.9737 - val_loss: 0.6326 - val_acc: 0.8707\n",
      "Epoch 79/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0692 - acc: 0.9747 - val_loss: 0.6421 - val_acc: 0.8734\n",
      "Epoch 80/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0667 - acc: 0.9767 - val_loss: 0.7232 - val_acc: 0.8678\n",
      "Epoch 81/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0684 - acc: 0.9751 - val_loss: 0.6852 - val_acc: 0.8662\n",
      "Epoch 82/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0673 - acc: 0.9760 - val_loss: 0.6474 - val_acc: 0.8795\n",
      "Epoch 83/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0699 - acc: 0.9750 - val_loss: 0.6209 - val_acc: 0.8763\n",
      "Epoch 84/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0658 - acc: 0.9768 - val_loss: 0.6651 - val_acc: 0.8745\n",
      "Epoch 85/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0673 - acc: 0.9763 - val_loss: 0.6108 - val_acc: 0.8845\n",
      "Epoch 86/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0650 - acc: 0.9764 - val_loss: 0.7356 - val_acc: 0.8709\n",
      "Epoch 87/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0638 - acc: 0.9769 - val_loss: 0.7636 - val_acc: 0.8568\n",
      "Epoch 88/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0618 - acc: 0.9785 - val_loss: 0.6768 - val_acc: 0.8757\n",
      "Epoch 89/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0662 - acc: 0.9766 - val_loss: 0.6861 - val_acc: 0.8725\n",
      "Epoch 90/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0602 - acc: 0.9792 - val_loss: 0.7193 - val_acc: 0.8662\n",
      "Epoch 91/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0597 - acc: 0.9788 - val_loss: 0.6388 - val_acc: 0.8758\n",
      "Epoch 92/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0623 - acc: 0.9770 - val_loss: 0.8178 - val_acc: 0.8589\n",
      "Epoch 93/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0604 - acc: 0.9780 - val_loss: 0.6797 - val_acc: 0.8748\n",
      "Epoch 94/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0598 - acc: 0.9790 - val_loss: 0.7425 - val_acc: 0.8614\n",
      "Epoch 95/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0594 - acc: 0.9802 - val_loss: 0.9052 - val_acc: 0.8424\n",
      "Epoch 96/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0595 - acc: 0.9784 - val_loss: 0.7054 - val_acc: 0.8717\n",
      "Epoch 97/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0583 - acc: 0.9796 - val_loss: 0.7124 - val_acc: 0.8669\n",
      "Epoch 98/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0568 - acc: 0.9799 - val_loss: 0.6385 - val_acc: 0.8786\n",
      "Epoch 99/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0585 - acc: 0.9789 - val_loss: 0.7414 - val_acc: 0.8608\n",
      "Epoch 100/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0603 - acc: 0.9787 - val_loss: 0.6617 - val_acc: 0.8749\n",
      "Epoch 101/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0569 - acc: 0.9797 - val_loss: 0.7034 - val_acc: 0.8652\n",
      "Epoch 102/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0529 - acc: 0.9818 - val_loss: 0.6094 - val_acc: 0.8850\n",
      "Epoch 103/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0547 - acc: 0.9801 - val_loss: 0.6958 - val_acc: 0.8725\n",
      "Epoch 104/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0549 - acc: 0.9805 - val_loss: 0.6013 - val_acc: 0.8844\n",
      "Epoch 105/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0512 - acc: 0.9824 - val_loss: 0.8054 - val_acc: 0.8532\n",
      "Epoch 106/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0528 - acc: 0.9810 - val_loss: 0.6482 - val_acc: 0.8784\n",
      "Epoch 107/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0542 - acc: 0.9810 - val_loss: 0.6801 - val_acc: 0.8726\n",
      "Epoch 108/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0532 - acc: 0.9810 - val_loss: 0.6653 - val_acc: 0.8737\n",
      "Epoch 109/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0528 - acc: 0.9813 - val_loss: 0.6365 - val_acc: 0.8811\n",
      "Epoch 110/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0502 - acc: 0.9825 - val_loss: 0.6380 - val_acc: 0.8820\n",
      "Epoch 111/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0499 - acc: 0.9826 - val_loss: 0.7540 - val_acc: 0.8658\n",
      "Epoch 112/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0561 - acc: 0.9805 - val_loss: 0.8348 - val_acc: 0.8567\n",
      "Epoch 113/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0490 - acc: 0.9822 - val_loss: 0.8637 - val_acc: 0.8484\n",
      "Epoch 114/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0506 - acc: 0.9824 - val_loss: 0.6643 - val_acc: 0.8772\n",
      "Epoch 115/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0476 - acc: 0.9827 - val_loss: 0.5959 - val_acc: 0.8869\n",
      "Epoch 116/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0513 - acc: 0.9823 - val_loss: 0.6713 - val_acc: 0.8785\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0481 - acc: 0.9831 - val_loss: 0.7432 - val_acc: 0.8661\n",
      "Epoch 118/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0507 - acc: 0.9817 - val_loss: 0.6430 - val_acc: 0.8828\n",
      "Epoch 119/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0459 - acc: 0.9835 - val_loss: 0.6350 - val_acc: 0.8865\n",
      "Epoch 120/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0456 - acc: 0.9833 - val_loss: 0.7789 - val_acc: 0.8620\n",
      "Epoch 121/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0498 - acc: 0.9830 - val_loss: 0.6552 - val_acc: 0.8818\n",
      "Epoch 122/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0462 - acc: 0.9838 - val_loss: 0.7062 - val_acc: 0.8764\n",
      "Epoch 123/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0454 - acc: 0.9840 - val_loss: 0.7320 - val_acc: 0.8735\n",
      "Epoch 124/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0501 - acc: 0.9828 - val_loss: 0.6883 - val_acc: 0.8803\n",
      "Epoch 125/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0433 - acc: 0.9852 - val_loss: 0.7312 - val_acc: 0.8708\n",
      "Epoch 126/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0443 - acc: 0.9844 - val_loss: 0.7379 - val_acc: 0.8727\n",
      "Epoch 127/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0431 - acc: 0.9842 - val_loss: 0.6473 - val_acc: 0.8859\n",
      "Epoch 128/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0470 - acc: 0.9833 - val_loss: 0.6351 - val_acc: 0.8809\n",
      "Epoch 129/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0459 - acc: 0.9838 - val_loss: 0.6662 - val_acc: 0.8844\n",
      "Epoch 130/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0430 - acc: 0.9848 - val_loss: 0.7606 - val_acc: 0.8747\n",
      "Epoch 131/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0453 - acc: 0.9838 - val_loss: 0.8164 - val_acc: 0.8666\n",
      "Epoch 132/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0434 - acc: 0.9841 - val_loss: 0.6677 - val_acc: 0.8771\n",
      "Epoch 133/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0397 - acc: 0.9859 - val_loss: 0.7699 - val_acc: 0.8697\n",
      "Epoch 134/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0418 - acc: 0.9855 - val_loss: 0.6354 - val_acc: 0.8899\n",
      "Epoch 135/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0435 - acc: 0.9854 - val_loss: 0.7651 - val_acc: 0.8628\n",
      "Epoch 136/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0414 - acc: 0.9851 - val_loss: 0.6732 - val_acc: 0.8824\n",
      "Epoch 137/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0452 - acc: 0.9842 - val_loss: 0.7733 - val_acc: 0.8736\n",
      "Epoch 138/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0407 - acc: 0.9857 - val_loss: 0.6947 - val_acc: 0.8802\n",
      "Epoch 139/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0429 - acc: 0.9848 - val_loss: 0.7016 - val_acc: 0.8814\n",
      "Epoch 140/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0391 - acc: 0.9862 - val_loss: 0.6361 - val_acc: 0.8877\n",
      "Epoch 141/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0444 - acc: 0.9834 - val_loss: 0.6572 - val_acc: 0.8823\n",
      "Epoch 142/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0401 - acc: 0.9863 - val_loss: 0.7593 - val_acc: 0.8761\n",
      "Epoch 143/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0436 - acc: 0.9852 - val_loss: 0.6044 - val_acc: 0.8877\n",
      "Epoch 144/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0392 - acc: 0.9869 - val_loss: 0.6763 - val_acc: 0.8810\n",
      "Epoch 145/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0377 - acc: 0.9867 - val_loss: 0.6935 - val_acc: 0.8794\n",
      "Epoch 146/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0411 - acc: 0.9859 - val_loss: 0.6792 - val_acc: 0.8707\n",
      "Epoch 147/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0403 - acc: 0.9859 - val_loss: 0.6978 - val_acc: 0.8808\n",
      "Epoch 148/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0381 - acc: 0.9867 - val_loss: 0.8018 - val_acc: 0.8645\n",
      "Epoch 149/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0413 - acc: 0.9854 - val_loss: 0.6466 - val_acc: 0.8844\n",
      "Epoch 150/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0386 - acc: 0.9865 - val_loss: 0.7607 - val_acc: 0.8739\n",
      "Epoch 151/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0375 - acc: 0.9866 - val_loss: 0.8895 - val_acc: 0.8524\n",
      "Epoch 152/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0401 - acc: 0.9860 - val_loss: 0.8406 - val_acc: 0.8575\n",
      "Epoch 153/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0388 - acc: 0.9866 - val_loss: 0.8073 - val_acc: 0.8619\n",
      "Epoch 154/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0378 - acc: 0.9870 - val_loss: 0.7208 - val_acc: 0.8787\n",
      "Epoch 155/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0369 - acc: 0.9870 - val_loss: 0.7383 - val_acc: 0.8773\n",
      "Epoch 156/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0376 - acc: 0.9870 - val_loss: 0.6613 - val_acc: 0.8864\n",
      "Epoch 157/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0376 - acc: 0.9866 - val_loss: 0.7378 - val_acc: 0.8769\n",
      "Epoch 158/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0375 - acc: 0.9873 - val_loss: 0.7373 - val_acc: 0.8779\n",
      "Epoch 159/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0365 - acc: 0.9874 - val_loss: 0.6868 - val_acc: 0.8822\n",
      "Epoch 160/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0348 - acc: 0.9879 - val_loss: 0.8399 - val_acc: 0.8687\n",
      "Epoch 161/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0364 - acc: 0.9880 - val_loss: 0.6658 - val_acc: 0.8849\n",
      "Epoch 162/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0398 - acc: 0.9860 - val_loss: 0.7514 - val_acc: 0.8772\n",
      "Epoch 163/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0332 - acc: 0.9882 - val_loss: 0.6747 - val_acc: 0.8828\n",
      "Epoch 164/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0364 - acc: 0.9875 - val_loss: 0.6824 - val_acc: 0.8880\n",
      "Epoch 165/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0360 - acc: 0.9874 - val_loss: 0.8354 - val_acc: 0.8594\n",
      "Epoch 166/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0341 - acc: 0.9882 - val_loss: 0.7018 - val_acc: 0.8843\n",
      "Epoch 167/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0388 - acc: 0.9867 - val_loss: 0.6553 - val_acc: 0.8855\n",
      "Epoch 168/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0353 - acc: 0.9872 - val_loss: 0.6500 - val_acc: 0.8895\n",
      "Epoch 169/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0311 - acc: 0.9893 - val_loss: 0.6856 - val_acc: 0.8834\n",
      "Epoch 170/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0320 - acc: 0.9889 - val_loss: 0.6714 - val_acc: 0.8843\n",
      "Epoch 171/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0370 - acc: 0.9870 - val_loss: 0.6685 - val_acc: 0.8852\n",
      "Epoch 172/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0375 - acc: 0.9869 - val_loss: 0.6650 - val_acc: 0.8877\n",
      "Epoch 173/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0343 - acc: 0.9884 - val_loss: 0.6209 - val_acc: 0.8891\n",
      "Epoch 174/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0330 - acc: 0.9882 - val_loss: 0.6673 - val_acc: 0.8826\n",
      "Epoch 175/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0329 - acc: 0.9886 - val_loss: 0.6633 - val_acc: 0.8855\n",
      "Epoch 176/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0342 - acc: 0.9879 - val_loss: 0.6495 - val_acc: 0.8879\n",
      "Epoch 177/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0371 - acc: 0.9875 - val_loss: 0.7856 - val_acc: 0.8623\n",
      "Epoch 178/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0308 - acc: 0.9895 - val_loss: 0.6793 - val_acc: 0.8812\n",
      "Epoch 179/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0367 - acc: 0.9868 - val_loss: 0.7362 - val_acc: 0.8757\n",
      "Epoch 180/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0309 - acc: 0.9893 - val_loss: 0.6876 - val_acc: 0.8810\n",
      "Epoch 181/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0316 - acc: 0.9896 - val_loss: 0.6930 - val_acc: 0.8829\n",
      "Epoch 182/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0308 - acc: 0.9896 - val_loss: 0.6399 - val_acc: 0.8935\n",
      "Epoch 183/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0350 - acc: 0.9871 - val_loss: 0.7397 - val_acc: 0.8763\n",
      "Epoch 184/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0321 - acc: 0.9888 - val_loss: 0.6936 - val_acc: 0.8821\n",
      "Epoch 185/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0323 - acc: 0.9892 - val_loss: 0.7075 - val_acc: 0.8881\n",
      "Epoch 186/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0324 - acc: 0.9891 - val_loss: 0.6337 - val_acc: 0.8905\n",
      "Epoch 187/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0303 - acc: 0.9893 - val_loss: 0.6730 - val_acc: 0.8803\n",
      "Epoch 188/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0297 - acc: 0.9895 - val_loss: 0.6271 - val_acc: 0.8889\n",
      "Epoch 189/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0331 - acc: 0.9884 - val_loss: 0.6628 - val_acc: 0.8848\n",
      "Epoch 190/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0314 - acc: 0.9890 - val_loss: 0.6566 - val_acc: 0.8859\n",
      "Epoch 191/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0321 - acc: 0.9884 - val_loss: 0.7094 - val_acc: 0.8829\n",
      "Epoch 192/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0329 - acc: 0.9887 - val_loss: 0.6373 - val_acc: 0.8856\n",
      "Epoch 193/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0307 - acc: 0.9892 - val_loss: 0.7503 - val_acc: 0.8767\n",
      "Epoch 194/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0332 - acc: 0.9883 - val_loss: 0.6945 - val_acc: 0.8875\n",
      "Epoch 195/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0326 - acc: 0.9892 - val_loss: 0.6297 - val_acc: 0.8874\n",
      "Epoch 196/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0289 - acc: 0.9900 - val_loss: 0.7013 - val_acc: 0.8845\n",
      "Epoch 197/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0261 - acc: 0.9910 - val_loss: 0.7268 - val_acc: 0.8882\n",
      "Epoch 198/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0314 - acc: 0.9893 - val_loss: 0.6874 - val_acc: 0.8876\n",
      "Epoch 199/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0308 - acc: 0.9899 - val_loss: 0.7886 - val_acc: 0.8725\n",
      "Epoch 200/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0304 - acc: 0.9889 - val_loss: 0.7431 - val_acc: 0.8815\n",
      "Epoch 201/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0311 - acc: 0.9888 - val_loss: 0.6084 - val_acc: 0.8931\n",
      "Epoch 202/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0308 - acc: 0.9892 - val_loss: 0.6591 - val_acc: 0.8877\n",
      "Epoch 203/300\n",
      "50000/50000 [==============================] - 212s 4ms/step - loss: 0.0282 - acc: 0.9905 - val_loss: 0.6366 - val_acc: 0.8904\n",
      "Epoch 204/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0294 - acc: 0.9895 - val_loss: 0.7223 - val_acc: 0.8833\n",
      "Epoch 205/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0295 - acc: 0.9893 - val_loss: 0.6665 - val_acc: 0.8913\n",
      "Epoch 206/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0299 - acc: 0.9897 - val_loss: 0.7442 - val_acc: 0.8812\n",
      "Epoch 207/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0308 - acc: 0.9896 - val_loss: 0.6828 - val_acc: 0.8837\n",
      "Epoch 208/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0288 - acc: 0.9895 - val_loss: 0.6908 - val_acc: 0.8849\n",
      "Epoch 209/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0289 - acc: 0.9902 - val_loss: 0.6623 - val_acc: 0.8909\n",
      "Epoch 210/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0288 - acc: 0.9898 - val_loss: 0.6458 - val_acc: 0.8854\n",
      "Epoch 211/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0299 - acc: 0.9897 - val_loss: 0.7020 - val_acc: 0.8789\n",
      "Epoch 212/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0278 - acc: 0.9900 - val_loss: 0.6669 - val_acc: 0.8895\n",
      "Epoch 213/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0319 - acc: 0.9887 - val_loss: 0.7067 - val_acc: 0.8882\n",
      "Epoch 214/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0263 - acc: 0.9904 - val_loss: 0.6817 - val_acc: 0.8880\n",
      "Epoch 215/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0265 - acc: 0.9905 - val_loss: 0.7745 - val_acc: 0.8781\n",
      "Epoch 216/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0289 - acc: 0.9902 - val_loss: 0.7640 - val_acc: 0.8802\n",
      "Epoch 217/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0296 - acc: 0.9896 - val_loss: 0.6684 - val_acc: 0.8912\n",
      "Epoch 218/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0284 - acc: 0.9902 - val_loss: 0.6743 - val_acc: 0.8873\n",
      "Epoch 219/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0267 - acc: 0.9910 - val_loss: 0.6572 - val_acc: 0.8844\n",
      "Epoch 220/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0284 - acc: 0.9901 - val_loss: 0.6380 - val_acc: 0.8887\n",
      "Epoch 221/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0264 - acc: 0.9907 - val_loss: 0.7169 - val_acc: 0.8850\n",
      "Epoch 222/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0255 - acc: 0.9908 - val_loss: 0.6484 - val_acc: 0.8914\n",
      "Epoch 223/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0258 - acc: 0.9913 - val_loss: 0.7546 - val_acc: 0.8784\n",
      "Epoch 224/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0290 - acc: 0.9896 - val_loss: 0.6574 - val_acc: 0.8888\n",
      "Epoch 225/300\n",
      "50000/50000 [==============================] - 205s 4ms/step - loss: 0.0279 - acc: 0.9904 - val_loss: 0.6948 - val_acc: 0.8866\n",
      "Epoch 226/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0278 - acc: 0.9904 - val_loss: 0.6676 - val_acc: 0.8865\n",
      "Epoch 227/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0277 - acc: 0.9904 - val_loss: 0.6855 - val_acc: 0.8844\n",
      "Epoch 228/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0278 - acc: 0.9905 - val_loss: 0.6689 - val_acc: 0.8896\n",
      "Epoch 229/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0273 - acc: 0.9901 - val_loss: 0.7331 - val_acc: 0.8813\n",
      "Epoch 230/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0261 - acc: 0.9910 - val_loss: 0.7889 - val_acc: 0.8779\n",
      "Epoch 231/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0248 - acc: 0.9913 - val_loss: 0.6319 - val_acc: 0.8963\n",
      "Epoch 232/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0278 - acc: 0.9908 - val_loss: 0.6317 - val_acc: 0.8938\n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0255 - acc: 0.9910 - val_loss: 0.6916 - val_acc: 0.8879\n",
      "Epoch 234/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0233 - acc: 0.9921 - val_loss: 0.6573 - val_acc: 0.8927\n",
      "Epoch 235/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0270 - acc: 0.9909 - val_loss: 0.6687 - val_acc: 0.8901\n",
      "Epoch 236/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0256 - acc: 0.9910 - val_loss: 0.6249 - val_acc: 0.8959\n",
      "Epoch 237/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0256 - acc: 0.9911 - val_loss: 0.6467 - val_acc: 0.8925\n",
      "Epoch 238/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0260 - acc: 0.9910 - val_loss: 0.6369 - val_acc: 0.8946\n",
      "Epoch 239/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0253 - acc: 0.9910 - val_loss: 0.6659 - val_acc: 0.8895\n",
      "Epoch 240/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0273 - acc: 0.9908 - val_loss: 0.7883 - val_acc: 0.8749\n",
      "Epoch 241/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0277 - acc: 0.9906 - val_loss: 0.6268 - val_acc: 0.8923\n",
      "Epoch 242/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0237 - acc: 0.9915 - val_loss: 0.6305 - val_acc: 0.8929\n",
      "Epoch 243/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0255 - acc: 0.9913 - val_loss: 0.6667 - val_acc: 0.8859\n",
      "Epoch 244/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0240 - acc: 0.9919 - val_loss: 0.6834 - val_acc: 0.8908\n",
      "Epoch 245/300\n",
      "50000/50000 [==============================] - 205s 4ms/step - loss: 0.0234 - acc: 0.9919 - val_loss: 0.7571 - val_acc: 0.8762\n",
      "Epoch 246/300\n",
      "50000/50000 [==============================] - 207s 4ms/step - loss: 0.0258 - acc: 0.9911 - val_loss: 0.6836 - val_acc: 0.8872\n",
      "Epoch 247/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0256 - acc: 0.9905 - val_loss: 0.7130 - val_acc: 0.8866\n",
      "Epoch 248/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0276 - acc: 0.9906 - val_loss: 0.7009 - val_acc: 0.8887\n",
      "Epoch 249/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0255 - acc: 0.9911 - val_loss: 0.7541 - val_acc: 0.8790\n",
      "Epoch 250/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0257 - acc: 0.9911 - val_loss: 0.6392 - val_acc: 0.8957\n",
      "Epoch 251/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0274 - acc: 0.9904 - val_loss: 0.6509 - val_acc: 0.8906\n",
      "Epoch 252/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0246 - acc: 0.9913 - val_loss: 0.6180 - val_acc: 0.8973\n",
      "Epoch 253/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0250 - acc: 0.9912 - val_loss: 0.7401 - val_acc: 0.8820\n",
      "Epoch 254/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0258 - acc: 0.9908 - val_loss: 0.6643 - val_acc: 0.8920\n",
      "Epoch 255/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0215 - acc: 0.9928 - val_loss: 0.6295 - val_acc: 0.8983\n",
      "Epoch 256/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0225 - acc: 0.9923 - val_loss: 0.6696 - val_acc: 0.8898\n",
      "Epoch 257/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0253 - acc: 0.9912 - val_loss: 0.6492 - val_acc: 0.8908\n",
      "Epoch 258/300\n",
      "50000/50000 [==============================] - 207s 4ms/step - loss: 0.0246 - acc: 0.9912 - val_loss: 0.7296 - val_acc: 0.8885\n",
      "Epoch 259/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0252 - acc: 0.9912 - val_loss: 0.6852 - val_acc: 0.8907\n",
      "Epoch 260/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0234 - acc: 0.9921 - val_loss: 0.6937 - val_acc: 0.8886\n",
      "Epoch 261/300\n",
      "50000/50000 [==============================] - 207s 4ms/step - loss: 0.0243 - acc: 0.9915 - val_loss: 0.6505 - val_acc: 0.8884\n",
      "Epoch 262/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0250 - acc: 0.9913 - val_loss: 0.6051 - val_acc: 0.8954\n",
      "Epoch 263/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0217 - acc: 0.9924 - val_loss: 0.6904 - val_acc: 0.8915\n",
      "Epoch 264/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0266 - acc: 0.9910 - val_loss: 0.7133 - val_acc: 0.8862\n",
      "Epoch 265/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0262 - acc: 0.9912 - val_loss: 0.6776 - val_acc: 0.8860\n",
      "Epoch 266/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0230 - acc: 0.9920 - val_loss: 0.6508 - val_acc: 0.8926\n",
      "Epoch 267/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0222 - acc: 0.9927 - val_loss: 0.6733 - val_acc: 0.8927\n",
      "Epoch 268/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0233 - acc: 0.9920 - val_loss: 0.7479 - val_acc: 0.8823\n",
      "Epoch 269/300\n",
      "50000/50000 [==============================] - 207s 4ms/step - loss: 0.0224 - acc: 0.9923 - val_loss: 0.6254 - val_acc: 0.8960\n",
      "Epoch 270/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0251 - acc: 0.9913 - val_loss: 0.6490 - val_acc: 0.8905\n",
      "Epoch 271/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0225 - acc: 0.9925 - val_loss: 0.7591 - val_acc: 0.8820\n",
      "Epoch 272/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0217 - acc: 0.9919 - val_loss: 0.6838 - val_acc: 0.8902\n",
      "Epoch 273/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0243 - acc: 0.9914 - val_loss: 0.7442 - val_acc: 0.8823\n",
      "Epoch 274/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0215 - acc: 0.9924 - val_loss: 0.6655 - val_acc: 0.8922\n",
      "Epoch 275/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0226 - acc: 0.9920 - val_loss: 0.7398 - val_acc: 0.8822\n",
      "Epoch 276/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0239 - acc: 0.9917 - val_loss: 0.8044 - val_acc: 0.8822\n",
      "Epoch 277/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0244 - acc: 0.9914 - val_loss: 0.7164 - val_acc: 0.8890\n",
      "Epoch 278/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0229 - acc: 0.9921 - val_loss: 0.7117 - val_acc: 0.8860\n",
      "Epoch 279/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0248 - acc: 0.9919 - val_loss: 0.7276 - val_acc: 0.8838\n",
      "Epoch 280/300\n",
      "50000/50000 [==============================] - 207s 4ms/step - loss: 0.0242 - acc: 0.9914 - val_loss: 0.7085 - val_acc: 0.8860\n",
      "Epoch 281/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0207 - acc: 0.9934 - val_loss: 0.7440 - val_acc: 0.8848\n",
      "Epoch 282/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0203 - acc: 0.9929 - val_loss: 0.6689 - val_acc: 0.8940\n",
      "Epoch 283/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0228 - acc: 0.9923 - val_loss: 0.6340 - val_acc: 0.8936\n",
      "Epoch 284/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0245 - acc: 0.9916 - val_loss: 0.6316 - val_acc: 0.8923\n",
      "Epoch 285/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0218 - acc: 0.9924 - val_loss: 0.6663 - val_acc: 0.8929\n",
      "Epoch 286/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0218 - acc: 0.9925 - val_loss: 0.6778 - val_acc: 0.8935\n",
      "Epoch 287/300\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0205 - acc: 0.9932 - val_loss: 0.7065 - val_acc: 0.8885\n",
      "Epoch 288/300\n",
      "50000/50000 [==============================] - 207s 4ms/step - loss: 0.0226 - acc: 0.9922 - val_loss: 0.6646 - val_acc: 0.8944\n",
      "Epoch 289/300\n",
      "50000/50000 [==============================] - 207s 4ms/step - loss: 0.0235 - acc: 0.9921 - val_loss: 0.6916 - val_acc: 0.8925\n",
      "Epoch 290/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0206 - acc: 0.9926 - val_loss: 0.6601 - val_acc: 0.8910\n",
      "Epoch 291/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0250 - acc: 0.9918 - val_loss: 0.6293 - val_acc: 0.8974\n",
      "Epoch 292/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0209 - acc: 0.9929 - val_loss: 0.6736 - val_acc: 0.8940\n",
      "Epoch 293/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0229 - acc: 0.9916 - val_loss: 0.6580 - val_acc: 0.8921\n",
      "Epoch 294/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0235 - acc: 0.9924 - val_loss: 0.6371 - val_acc: 0.8920\n",
      "Epoch 295/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0204 - acc: 0.9929 - val_loss: 0.6792 - val_acc: 0.8907\n",
      "Epoch 296/300\n",
      "50000/50000 [==============================] - 206s 4ms/step - loss: 0.0189 - acc: 0.9936 - val_loss: 0.7049 - val_acc: 0.8907\n",
      "Epoch 297/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0238 - acc: 0.9916 - val_loss: 0.6880 - val_acc: 0.8887\n",
      "Epoch 298/300\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 0.0196 - acc: 0.9932 - val_loss: 0.7237 - val_acc: 0.8891\n",
      "Epoch 299/300\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 0.0199 - acc: 0.9936 - val_loss: 0.6757 - val_acc: 0.8880\n",
      "Epoch 300/300\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 0.0240 - acc: 0.9917 - val_loss: 0.6220 - val_acc: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12dcea828>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8972\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "'''\n",
    "Test accuracy: 0.8972\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 298/300\n",
    "50000/50000 [==============================] - 91s 2ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 1.0791 - val_acc: 0.8551\n",
    "Epoch 299/300\n",
    "50000/50000 [==============================] - 91s 2ms/step - loss: 0.0027 - acc: 0.9991 - val_loss: 1.1468 - val_acc: 0.8472\n",
    "Epoch 300/300\n",
    "50000/50000 [==============================] - 91s 2ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 1.2931 - val_acc: 0.8339\n",
    "Test loss: 1.2930997592\n",
    "Test accuracy: 0.8339"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
