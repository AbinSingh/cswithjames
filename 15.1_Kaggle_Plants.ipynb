{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start building and training the Neural Network we have to understand about this Dataset\n",
    "\n",
    "How many classes and what is going to be a input size and have to set the strategy of training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.image as img\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = \"plants/raw/\"\n",
    "file_list = os.listdir(raw_dir)\n",
    "class_name = []\n",
    "for name in file_list:\n",
    "    if not name.startswith('.'):\n",
    "        class_name.append(name)\n",
    "        \n",
    "# for name in class_name:\n",
    "#     print(name)\n",
    "    \n",
    "num_classes = len(class_name)\n",
    "# print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Code above I could found out that this dataset is consist of 12 classes.\n",
    "\n",
    "In the next step we are going to check if all the classes have same number of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for name in class_name:\n",
    "    temp = []\n",
    "    location = raw_dir + name\n",
    "    #print(location)\n",
    "    data_list = os.listdir(location)\n",
    "    \n",
    "    for file_name in data_list:\n",
    "        if file_name.endswith('.png'):\n",
    "            temp.append(file_name)\n",
    " \n",
    "    print(len(temp))\n",
    "    length.append([len(temp)])\n",
    "    \n",
    "# print(\"AVG : \", np.mean(length))\n",
    "# print(\"Max : \", np.max(length))\n",
    "# print(\"Avg + Max : \",np.mean(length)+np.max(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code above I could be able to find out that classes have different amount of data in each categories. \n",
    "\n",
    "We have to use data augmentation to match the number of data in each categories\n",
    "\n",
    "The Data augmentation is the way to make the training dataset larger with the limited number of data.\n",
    "\n",
    "In this stage I have to decide what is going to be the target number that want to be augmented.\n",
    "\n",
    "There is no answer to this question. However, since this dataset is so small I will target 1000 to be the final data size for each class\n",
    "\n",
    "Number of Class : 12\n",
    "\n",
    "Data for each classes : 1,000\n",
    "\n",
    "total number of data : 12,000\n",
    "\n",
    "Training set : 0.9, Test set : 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Data augmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 1000\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "for name in class_name:\n",
    "    print(\"Start Augmenting \" + name)\n",
    "#     print(name)\n",
    "    class_dir = raw_dir + name +\"/\"\n",
    "#     print(class_dir)\n",
    "\n",
    "    file_list = os.listdir(class_dir)\n",
    "#     print(len(file_list))\n",
    "    \n",
    "    num_augment = ceil((target-len(file_list)) / len(file_list))\n",
    "    \n",
    "#     print(num_augment)\n",
    "\n",
    "    if(num_augment == 0):\n",
    "        continue\n",
    "        \n",
    "    for file_name in file_list:\n",
    "        if not file_name.endswith(\".png\"):\n",
    "            continue\n",
    "            \n",
    "        file_dir = class_dir + file_name\n",
    "        image = img.imread(file_dir)\n",
    "        x = img_to_array(image) \n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        if x.shape[3] != 3:\n",
    "            continue\n",
    "#         print(file_name, x.shape)\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1,\n",
    "                                  save_to_dir=class_dir, save_prefix='aug_', save_format='png'):\n",
    "            i += 1\n",
    "            if i >= num_augment:\n",
    "                break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished augmenting the Dataset.\n",
    "\n",
    "If you want to you might want to make different directory for the augmented data\n",
    "\n",
    "but since the dataset is so small i didn't do that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
